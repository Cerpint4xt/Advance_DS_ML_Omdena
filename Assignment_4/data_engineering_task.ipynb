{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Task Homework\n",
    "## Name: Raul Antonio Catacora Grundy\n",
    "## Repo for this code : https://github.com/Cerpint4xt/Advance_DS_ML_Omdena/tree/homework_4_development\n",
    "\n",
    "## Objective:\n",
    "\n",
    "The objective of this assignment is to create a data engineering pipeline in Python and MySQL that involves ingesting data from a CSV file, preprocessing the data, storing it into a database, and creating a pipeline for this process.\n",
    "\n",
    "### Assignment Description:\n",
    "You are provided with a CSV file containing sample data. Your task is to create a Jupyter Notebook that performs the following steps:\n",
    "\n",
    "* Ingest the data from the CSV file.\n",
    "\n",
    "* Preprocess the data (cleaning, transformation, etc.).\n",
    "\n",
    "* Store the preprocessed data into a MySQL database.\n",
    "\n",
    "* Create a data engineering pipeline to automate the above steps.\n",
    "\n",
    "### Data Description:\n",
    "\n",
    "* The CSV file contains sample data with multiple columns. It is the same data used in the class. \n",
    "\n",
    "### Submission Guidelines:\n",
    "\n",
    "Create a Jupyter Notebook containing Python code for the data engineering pipeline.\n",
    "\n",
    "Include comments and explanations throughout the notebook to explain the code and the steps performed.\n",
    "Save the CSV file and any other relevant files in the same directory as the notebook.\n",
    "\n",
    "Ensure that the notebook is well-structured and easy to follow.\n",
    "Test the code to ensure that it executes correctly without errors.\n",
    "\n",
    "### Submit the following files:\n",
    "* Jupyter Notebook (.ipynb file)\n",
    "* CSV file containing sample data\n",
    "* Any additional files or resources used in the assignment\n",
    "\n",
    "### Note:\n",
    "\n",
    "* You are free to use any data engineering tools and libraries available in Python (e.g., pandas, SQLAlchemy, etc.).\n",
    "* Ensure that your code follows best practices and is well-documented.\n",
    "* Plagiarism will not be tolerated, and any instances of plagiarism will result in penalties.\n",
    "\n",
    "### Assignment Deadline:\n",
    "15th March 2024\n",
    "Good luck! If you have any questions or need clarification, feel free to reach out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\rcata\\anaconda3\\envs\\adv_omdena\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\rcata\\anaconda3\\envs\\adv_omdena\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rcata\\anaconda3\\envs\\adv_omdena\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rcata\\anaconda3\\envs\\adv_omdena\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rcata\\anaconda3\\envs\\adv_omdena\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rcata\\anaconda3\\envs\\adv_omdena\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\rcata\\anaconda3\\envs\\adv_omdena\\lib\\site-packages (1.4.52)\n",
      "Requirement already satisfied: psycopg2-binary in c:\\users\\rcata\\anaconda3\\envs\\adv_omdena\\lib\\site-packages (2.9.9)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\rcata\\anaconda3\\envs\\adv_omdena\\lib\\site-packages (from sqlalchemy) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install sqlalchemy psycopg2-binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regards of the docker file it is added to be used without the installation process locally. \n",
    "\n",
    "First it is necessary to install docker and docker compose. In order to have the postgresql database available. After achiving this. using docker to run `docker-compose -p car_proj up` where `-p` is the flag to name the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the database created in the docker container\n",
    "engine = create_engine('postgresql://root:root@localhost:5432/car_prices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the object for the connection\n",
    "engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the connection to the db\n",
    "query = \"\"\"\n",
    "SELECT 1 as number;\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql(query, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the csv file the sample dataset\n",
    "df_cars = pd.read_csv('car_prices.csv', parse_dates=['year', 'saledate'])\n",
    "df_cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA on the dataset\n",
    "df_cars.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More EDA on the dataset\n",
    "df_cars.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cehcking types to parse correctly \n",
    "df_cars.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysing NaN\n",
    "df_cars.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing NaN in the dataset\n",
    "df_cars_cleaned = df_cars.dropna()\n",
    "df_cars_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse year in year column\n",
    "df_cars_cleaned['year'] = pd.to_datetime(df_cars_cleaned['year'], errors='coerce').dt.year\n",
    "df_cars_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the saledate column\n",
    "df_cars_cleaned['saledate'] = df_cars_cleaned['saledate'].str[:-6]\n",
    "df_cars_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "df_cars_cleaned['saledate'] = df_cars_cleaned['saledate'].apply(lambda x: datetime.strptime(x, \"%a %b %d %Y %H:%M:%S %Z%z\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars_cleaned['saledate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingesting to the db\n",
    "df_cars_cleaned.to_sql(name='car_prices_data', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the ingestion \n",
    "query = \"\"\"\n",
    "SELECT * FROM car_prices_data;\n",
    "\"\"\"\n",
    "pd.read_sql(query, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dagster dagster-webserver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a new Dagster project \n",
    "After dagster installation it is necesary to create a project. \n",
    "Using the `dagster project scaffold --name car_prices-project`. The project will be created with the folder structure with a single dagster code location. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installation it is necessary to install all dagster dependencies with `pip install -e \".[dev]\"`\n",
    "\n",
    "Finally `dagster dev` will lauch the dagster environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'solid' from 'dagster' (c:\\Users\\rcata\\anaconda3\\envs\\adv_omdena\\lib\\site-packages\\dagster\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# The following is the code added to a pipelines folder to schedule the process \u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdagster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m solid, pipeline, schedules, ScheduleDefinition, daily_schedule, ModeDefinition\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m      6\u001b[0m \u001b[38;5;129m@solid\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata_processing\u001b[39m(context):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Your data processing code here\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'solid' from 'dagster' (c:\\Users\\rcata\\anaconda3\\envs\\adv_omdena\\lib\\site-packages\\dagster\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# The following is the code added to a pipelines folder to schedule the process \n",
    "\n",
    "from dagster import solid, pipeline, schedules, ScheduleDefinition, daily_schedule, ModeDefinition\n",
    "from datetime import datetime\n",
    "\n",
    "@solid\n",
    "def data_processing(context):\n",
    "    # Your data processing code here\n",
    "    pass\n",
    "\n",
    "@pipeline(\n",
    "    mode_defs=[ModeDefinition(name=\"default\")]\n",
    ")\n",
    "def data_pipeline():\n",
    "    data_processing()\n",
    "\n",
    "daily_schedule = ScheduleDefinition(\n",
    "    name=\"daily_schedule\",\n",
    "    cron_schedule=\"0 0 * * *\",  # Run at midnight every day\n",
    "    pipeline_name=\"data_pipeline\",\n",
    "    execution_timezone=\"UTC\"\n",
    ")\n",
    "\n",
    "# This decorator registers the schedule with the repository containing the pipeline\n",
    "@schedules\n",
    "def define_schedules():\n",
    "    return [daily_schedule]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv_omdena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
